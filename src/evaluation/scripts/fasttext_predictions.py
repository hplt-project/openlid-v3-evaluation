#!/usr/bin/env python3

import json
import fasttext
import argparse
import sys

import regex
from huggingface_hub import hf_hub_download
from eval_datasets import load_flores_data, load_udhr_data
from glotlid_customlid import CustomLID

# Regex patterns for text preprocessing
# defines what we want to remove from string for langID
NONWORD_REPLACE_STR = r"[^\p{Word}\p{Zs}]|\d"  # either (not a word nor a space) or (is digit)
NONWORD_REPLACE_PATTERN = regex.compile(NONWORD_REPLACE_STR)
SPACE_PATTERN = regex.compile(r"\s\s+")  # squeezes sequential whitespace

FASTTEXT_PREFIX = "__label__"

def get_model_info(model_name):
    models = {
        "glotlid": ("cis-lmu/glotlid", "model.bin"),
        "openlid": ("laurievb/OpenLID", "model.bin"),
        "openlid-v2": ("laurievb/OpenLID-v2", "model.bin"),
        "retrained": (None, None)
    }

    if model_name not in models:
        raise ValueError(f"Unknown model: {model_name}. Available models: {list(models.keys())}")

    return models[model_name]


def load_language_list(languages_file_path):
    with open(languages_file_path, 'r') as f:
        language_labels = [line.strip() for line in f if line.strip()]
    return [f'__label__{label}' for label in language_labels]


def preprocess_text(text):
    text = text.replace('\n', ' ').strip().lower()
    text = regex.sub(NONWORD_REPLACE_PATTERN, "", text)
    text = regex.sub(SPACE_PATTERN, " ", text)
    return text


def predict_languages(dataset, model_name, split=None, languages_file=None, prediction_mode='before', enable_preprocessing=False, model_path=None):

    repo_id, filename = get_model_info(model_name)

    if model_path:
        print(f"Using provided model path: {model_path}...", file=sys.stderr)
    elif repo_id and filename:
        print(f"Downloading {model_name} model from Hugging Face...", file=sys.stderr)
        model_path = hf_hub_download(repo_id=repo_id, filename=filename)
    else:
        raise ValueError(f"No model path provided and no HuggingFace info for model: {model_name}")

    print(f"Loading model from {model_path}...", file=sys.stderr)

    # Load languages from file if provided
    languages_list = None
    if languages_file is not None:
        print(f"Loading languages from {languages_file}...", file=sys.stderr)
        languages_list = load_language_list(languages_file)
        print(f"Loaded {len(languages_list)} languages: {languages_list[:5]}{'...' if len(languages_list) > 5 else ''}", file=sys.stderr)

    if languages_list is not None:
        model = CustomLID(model_path, languages=languages_list, mode=prediction_mode)
    else:
        model = fasttext.load_model(model_path)

    non_flores = {
        "udhr", "fastspell", "setimes", "parlasent", "he_bcs_ge_full", "ITDI_2022", "hplt", "hrv_Latn-was-wrong",
        "bos_Latn-was-wrong", "srp_Cyrl-was-wrong", "nob_Latn-was-wrong"
    }
    if dataset == "flores":
        if split is None:
            raise ValueError("Split must be specified for FLORES+ dataset")
        data = load_flores_data(split)
    elif dataset in non_flores:
        data = load_udhr_data(path=dataset)
    else:
        raise ValueError(f"Unknown dataset: {dataset}. Available datasets: flores, {non_flores}")

    print(f"Processing {len(data)} examples...", file=sys.stderr)
    results = []
    if args.ensemble_with_glotlid_k:
        ensemble_model_path = hf_hub_download(repo_id="cis-lmu/glotlid", filename="model.bin")
        ensemble_model = fasttext.load_model(ensemble_model_path)

    for i, example in enumerate(data):
        if i % 10000 == 0:
            print(f"Processed {i}/{len(data)} examples...", file=sys.stderr)

        if dataset == "flores":
            text_content = example["text"]
        else:
            text_content = example["sentence"]

        if not enable_preprocessing and model_name in ["openlid", "openlid-v2", "retrained"]:
            print("!" * 80, file=sys.stderr)
            print("Warning! Disabled preprocessing for openLID model", file=sys.stderr)
            print("!" * 80, file=sys.stderr)

        if args.ensemble_with_glotlid_k:
            pred = ensemble_model.predict(text_content, k=args.ensemble_with_glotlid_k)[0]
            glotlid_langs = [pred_lang.removeprefix("__label__") for pred_lang in pred]

        if enable_preprocessing:
            text_content = preprocess_text(text_content)

        if languages_list is not None:
            pred_labels, pred_probs = model.predict(text_content, k=1)
            pred = pred_labels[0]
            pred_lang = pred.replace(FASTTEXT_PREFIX, "")
        else:
            pred, pred_probs = model.predict(text_content, k=1)
            if (pred_probs[0] > 0.5) or (not args.threshold):
                pred_lang = pred[0].replace(FASTTEXT_PREFIX, "")
            else:
                pred_lang = 'zxx_Zxxx'
            if args.ensemble_with_glotlid_k and (pred_lang not in glotlid_langs):
                pred_lang = 'zxx_Zxxx'
        result = example.copy()
        if "predictions" not in result:
            result["predictions"] = {}
        result["predictions"][model_name] = pred_lang
        results.append(result)
        if not args.out_path:
            print(pred_lang)

    print(f"Completed processing {len(results)} examples", file=sys.stderr)

    if args.out_path:
        with open(args.out_path, 'w') as writer:
            for result in results:
                writer.write(json.dumps(result, ensure_ascii=False)+'\n')


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run fastText-based language identification predictions")
    parser.add_argument("--dataset", choices=[
        "flores", "udhr", "fastspell", "setimes", "parlasent", "he_bcs_ge_full", "ITDI_2022", "hplt",
        "hrv_Latn-was-wrong", "bos_Latn-was-wrong", "srp_Cyrl-was-wrong", "nob_Latn-was-wrong"
    ], required=True,
                       help="Dataset to process (flores or udhr)")
    parser.add_argument("--model", choices=["glotlid", "openlid", "openlid-v2", "retrained"], required=True,
                       help="Model to use (glotlid, openlid, openlid-v2, or retrained)")
    parser.add_argument("--split", choices=["dev", "devtest"],
                       help="Data split to process (required for FLORES+ dataset)")
    parser.add_argument("--languages-file", type=str,
                       help="Path to file containing language labels (one per line, e.g., eng_Latn)")
    parser.add_argument("--prediction-mode", choices=["before", "after"], default="before",
                       help="Prediction mode for CustomLID: 'before' (limit before softmax) or 'after' (limit after softmax)")
    parser.add_argument("--enable-preprocessing", action="store_true",
                       help="Enable text preprocessing (lowercase, normalize spaces, remove non-word characters)")
    parser.add_argument("--model-path", type=str,
                       help="Path to local model file (required for retrained model)")
    parser.add_argument("--out_path", type=str, default="")
    parser.add_argument("--ensemble_with_glotlid_k", type=int, default=0)
    parser.add_argument("--threshold", action="store_true")
    args = parser.parse_args()

    if args.dataset == "flores" and args.split is None:
        parser.error("--split is required when --dataset is flores")

    predict_languages(args.dataset, args.model, args.split, args.languages_file, args.prediction_mode, args.enable_preprocessing, args.model_path)
